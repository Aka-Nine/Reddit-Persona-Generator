Reddit User Persona Generator
=============================

Overview:
---------
This project is designed to analyze a Reddit user's public activity and generate a detailed psychological and behavioral persona. It combines web scraping, NLP, sentiment analysis, and LLM-based inference to produce structured persona reports.

The tool was built for practical, automated analysis of online behavior, with real-world applications in user profiling, market research, and academic study.

---

How It Works:
-------------
- Reddit user activity (posts and comments) is fetched using the Reddit API via the `praw` library.
- The text data is cleaned, processed, and analyzed using several NLP libraries including `nltk`, `TextBlob`, and `VaderSentiment`.
- The processed text is then sent to an LLM (like Groq or Gemini) to generate a detailed user persona, which includes traits, motivations, habits, frustrations, and goals.
- The output is formatted and saved as a `.txt` report. Optionally, it can also be viewed in a web browser using a Streamlit-based interface.

---

Why These Libraries Were Used:
------------------------------

• praw:
  Used to interact with the Reddit API and fetch posts/comments from public user profiles. It's the most mature and widely used Python library for Reddit scraping.

• requests:
  Used for sending HTTP POST requests to the Groq LLM API for persona analysis.

• pandas, numpy:
  Used for basic data manipulation, especially when summarizing posts/comments, timestamps, and subreddits.

• nltk:
  Core NLP library used for sentence tokenization, part-of-speech tagging, and basic linguistic preprocessing.

• textblob:
  Built on top of NLTK. It provides an easy way to calculate polarity and subjectivity, making sentiment analysis more accessible.

• vaderSentiment:
  Provides sentiment scores that are tuned for social media and short-form text, making it ideal for Reddit content.

• scikit-learn:
  Used for potential text similarity or classification features. It's included for future extensibility and was helpful in some keyword extractions.

• spacy:
  Included for future use or more advanced NLP if needed. It's a robust and fast NLP engine but not strictly required unless upgrading to NER or dependency parsing.

• python-dotenv & python-decouple:
  These manage environment variables securely, separating API keys and configuration from the codebase.

• tqdm:
  Adds progress bars for long-running scraping tasks. It improves UX in the terminal by showing scraping/processing progress.

• jsonschema:
  Used to validate structured data formats, such as ensuring the persona report follows the expected schema.

• colorlog:
  Enhances logging by adding color-coded logs, making debugging and output reading much easier during development.

• ratelimit:
  Ensures we stay within Reddit's or Groq’s API rate limits by automatically sleeping between requests if needed.

• textstat (optional):
  Used for calculating readability scores (e.g., Flesch-Kincaid) which contribute to analyzing the user’s writing complexity.

• streamlit (optional):
  Enables a simple web-based interface to visualize persona reports interactively. Very useful for demonstrations and reviewers.

---

Project Structure:
------------------
- main.py – Entry point of the project; ties everything together.
- config.py – Central config for loading environment variables and validating keys.
- src/ – Contains all core modules (scraper, processor, analyzer, generator).
- templates/ – Persona output layout template.
- output/ – Stores generated persona `.txt` files.
- viewer_app.py – Optional Streamlit app for browser visualization.
- .env.example – Template file for environment config (safe to share).

---

Design Philosophy:
------------------
This project was built with modularity and extensibility in mind. Each component—scraping, processing, analysis, generation—is isolated for clarity, testing, and reuse.

Special care was taken to follow PEP-8, use meaningful logging, and maintain clean exception handling.

The project also emphasizes responsible use of AI: the personas are approximations, not psychological evaluations, and should not be interpreted as absolute truths.

---

Getting Started:
----------------
1. Clone the repo and install dependencies.
2. Create a `.env` file using `.env.example` as a guide.
3. Run the script with a Reddit profile URL.
4. View the output text file in `/output/`, or launch the Streamlit app for a visual display.

---

Conclusion:
-----------
This project showcases how language models and NLP can be combined to create meaningful user insights from publicly available social data. It highlights ethical scraping, modular architecture, and practical use of modern LLM APIs.

